{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.5 사전학습된 BERT 모형의 직접 사용방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [허깅페이스](https://huggingface.co/transformers)\n",
    "- 자연어 분야에서 가장 널리 알려진 AI 커뮤니티.\n",
    "- 트랜스포머, BERT, GPT 등 거의 모든 자연어 처리 딥러닝 모형들을 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import pipeline\n",
    "pipeline('sentiment-analysis')   # 감성분석\n",
    "pipeline('text-classification')  # 문서분류\n",
    "pipeline('question-answering')   # 질의응답\n",
    "pipeline('text-generation')      # 문서생성\n",
    "pipeline('translation')          # 기계번역\n",
    "pipeline('summarization')        # 문서요약\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BERT는 기본적으로 많은 양의 메모리를 사용한다.\n",
    "- pipeline에서는 자동으로 토크나이저를 선택해 사용할 때 기본 옵션을 사용하는데 , 이 경우 토큰 수가 많아지면 토크나이저는 문제가 없지만 이후 모형을 돌릴 때 에러가 나는 경우가 많다. 따라서 긴 문서의 경우에는 pipeline을 사용하지 않고 직접 토크나이저와 모형을 사용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python_project\\파이썬 데이터분석 폴더\\python_textmining\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "d:\\python_project\\파이썬 데이터분석 폴더\\python_textmining\\venv\\lib\\site-packages\\torch\\cuda\\__init__.py:107: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감성분석 결과 : NEGATIVE, 감성스코어 : 0.9822\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "clf = pipeline(\"sentiment-analysis\")  # 감성분석\n",
    "result = clf(\"what a beauiful day!\")[0]\n",
    "print(\"감성분석 결과 : {}, 감성스코어 : {:.4f}\".format(result['label'], result['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.982161819934845}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf(\"what a beauiful day!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "d:\\python_project\\파이썬 데이터분석 폴더\\python_textmining\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1369: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank,  being overwhelmed by her own problems. In her mind, she was the next person to get caught, and was about to start making excuses for her sister when she\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generator = pipeline(\"text-generation\")  # 문장생성, GPT-2 사용\n",
    "result = text_generator(\"Alice was beginning to get very tired of sitting by her sister on the bank, \")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자동 클래스를 이용한 토크나이저와 모형의 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Auto Classes를 이용해 사전학습된 내용에 맞는 토크나이저와 모형을 자동으로 설정\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-cased-finetuned-mrpc\"\n",
    ")\n",
    "\n",
    "# 의미적으로 유사한 두 문장을 선언\n",
    "input_sentence = \"She angered me with her inappropriate comments, rumor-spreading, and disrespectfulness at the formal dinner table\"\n",
    "target_sentence = \"She made me angry when she was rude at dinner\"\n",
    "\n",
    "# 토큰화\n",
    "tokens = tokenizer(input_sentence, target_sentence, return_tensors=\"pt\")\n",
    "\n",
    "# 모형으로 결과를 예측\n",
    "logits = model(**tokens).logits  # softmax 적용하기 전의 신경망 결과\n",
    "\n",
    "# 소프트맥스를 이용해 결괏값을 클래스에 대한 확률로 변환\n",
    "results = torch.softmax(logits, dim=1).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no: 29%\n",
      "yes: 71%\n"
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(['no', 'yes']):\n",
    "    print(f\"{label}: {int(round(results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no: 95%\n",
      "yes: 5%\n"
     ]
    }
   ],
   "source": [
    "target_sentence = \"The boy quickly ran across the finish line, seizing yet another victory\"\n",
    "\n",
    "tokens = tokenizer(input_sentence, target_sentence, return_tensors=\"pt\")\n",
    "logits = model(**tokens).logits\n",
    "\n",
    "results = torch.softmax(logits, dim=1).tolist()[0]\n",
    "\n",
    "for i, label in enumerate(['no', 'yes']):\n",
    "    print(f\"{label}: {int(round(results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\namyo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set count : 1600\n",
      "Test set count : 400\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "# movie review data에서 file id를 가져옴\n",
    "fileids = movie_reviews.fileids()\n",
    "\n",
    "# file id를 이용해 raw text file을 가져옴\n",
    "reviews = [ movie_reviews.raw(fileid) for fileid in fileids ]\n",
    "categories = [ movie_reviews.categories(fileid)[0] for fileid in fileids ]\n",
    "\n",
    "# label을 0. 1의 값으로 변환\n",
    "label_dict = {'pos':1, 'neg':0}\n",
    "y = np.array([ label_dict[c] for c in categories ])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, y, test_size=0.2, random_state=7)\n",
    "\n",
    "print(f'Train set count : {len(X_train)}')\n",
    "print(f\"Test set count : {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK 영화리뷰 감성분석 정확도 : 0.8425\n"
     ]
    }
   ],
   "source": [
    "# pipeline을 사용하면 에러 발생 -> BERT를 사용\n",
    "# 테스트셋이 여러 개로 이뤄져서 배치로 처리\n",
    "# 많은 메모리를 사용하므로 400개의 테스트셋을 10개씩 잘라서 모형을 돌리고 결과를 합쳐서 성능을 살펴봄\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# cuda를 이용한 GPU연산이 가능하면 cuda를 사용하고, 아니면 cpu를 사용\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Auto Classes를 이용해 사전학습된 내용에 맞는 토크나이저와 모형을 자동으로 설정\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"  # 미세조정학습이 된 모형\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "# 모델을 gpu로 옮겨서 연산을 준비\n",
    "model = model.to(device)\n",
    "\n",
    "batch_size = 10  # 모형으로 한 번에 예측할 데이터의 수\n",
    "y_pred = []  # 전체 예측 결과를 저장\n",
    "\n",
    "num_batch = len(y_test) // batch_size\n",
    "\n",
    "for i in range(num_batch):\n",
    "    inputs = tokenizer(\n",
    "        X_test[i*batch_size: (i+1)*batch_size],  # X_test[0:10], X_test[10:20] ...\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 토큰화 결과를 GPU로 이동\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    # 모형으로 결과를 예측\n",
    "    logits = model(**inputs).logits\n",
    "    \n",
    "    # 결괏값을 클래스에 대한 확률로 변환\n",
    "    pred = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # 예측결과를 CPU로 가져와서 넘파이로 변환한 후,\n",
    "    # argmax로 확률이 가장 큰 클래스를 선택함.\n",
    "    results = pred.cpu().detach().numpy().argmax(axis=1)\n",
    "    \n",
    "    # 전체 예측결과에 추가\n",
    "    y_pred.extend(results.tolist())\n",
    "    \n",
    "# gpu 메모리를 비움\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "score = sum(y_test == np.array(y_pred)) / len(y_test)\n",
    "print(f\"NLTK 영화리뷰 감성분석 정확도 : {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
