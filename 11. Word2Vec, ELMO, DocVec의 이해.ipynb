{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Word2Vec - 대표적인 워드 임베딩 기법\n",
    "### 11.1.1 Word2Vec 학습의 원리\n",
    "- CBOW : 주변 단어를 이용해 중심에 있는 단어를 예측\n",
    "- skip-gram : 중심의 한 단어를 이용해 주변 단어를 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.2 Word2Vec 활용 - 학습된 모형 가져오기\n",
    "```python\n",
    "pip install --upgrade gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "d:\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.4SP5SUA7CBGXUEOC35YP2ASOICYYEQZZ.gfortran-win_amd64.dll\n",
      "d:\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.KeyedVectors"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 미리 학습된 gensim data를 다운로드\n",
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('glove-wiki-gigaword-50')  # 구글 뉴스 데이터를 학습한 Word2Vec 모델\n",
    "type(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Size of the vector: 50\n",
      "#Vector for king: [ 0.50451   0.68607  -0.59517  -0.022801  0.60046  -0.13498  -0.08813\n",
      "  0.47377  -0.61798  -0.31012  -0.076666  1.493    -0.034189 -0.98173\n",
      "  0.68229   0.81722  -0.51874  -0.31503  -0.55809   0.66421   0.1961\n",
      " -0.13495  -0.11476  -0.30344   0.41177  -2.223    -1.0756   -1.0783\n",
      " -0.34354   0.33505   1.9927   -0.04234  -0.64319   0.71125   0.49159\n",
      "  0.16754   0.34344  -0.25663  -0.8523    0.1661    0.40102   1.1685\n",
      " -1.0137   -0.21585  -0.15155   0.78321  -0.91241  -1.6106   -0.64426\n",
      " -0.51042 ]\n"
     ]
    }
   ],
   "source": [
    "vec_king = wv['king']\n",
    "print('#Size of the vector:', len(vec_king))\n",
    "print('#Vector for king:', vec_king)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53093773 vs 0.41133785\n",
      "0.53667 vs 0.60031056\n",
      "미니밴에 가까운 차: [('truck', 0.9100273251533508), ('suv', 0.904007613658905), ('jeep', 0.8619828820228577), ('vehicle', 0.8431736826896667), ('cars', 0.8421834111213684)]\n",
      "여성, 왕에는 가까우면서 남성과는 먼 단어: [('queen', 0.8523604273796082), ('throne', 0.7664334177970886), ('prince', 0.7592144012451172), ('daughter', 0.7473883628845215), ('elizabeth', 0.7460219860076904)]\n",
      "breakfast ccereal dinner lunch 중에서 다른 단어들과 거리가 가장 먼 단어 :  cereal\n"
     ]
    }
   ],
   "source": [
    "print(wv.similarity('king', 'man'), 'vs', wv.similarity('king', 'woman'))\n",
    "print(wv.similarity('queen', 'man'), 'vs', wv.similarity('queen', 'woman'))\n",
    "print('미니밴에 가까운 차:', wv.most_similar(positive=[\"car\", \"minivan\"], topn=5))\n",
    "print('여성, 왕에는 가까우면서 남성과는 먼 단어:', wv.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"], topn=5))\n",
    "print('breakfast ccereal dinner lunch 중에서 다른 단어들과 거리가 가장 먼 단어 : ', wv.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance between cat and dog : 0.08\n",
      "0.5375\n",
      "0.5627\n",
      "0.4377\n"
     ]
    }
   ],
   "source": [
    "print(\"distance between cat and dog : {:.2f}\".format(wv.distance('cat', 'dog')))  # 거리\n",
    "\n",
    "# 단어 집합 간의 유사도 계산\n",
    "print(\"{:.4f}\".format(wv.n_similarity(['bulgogi', 'shop'], ['japanese', 'restaurant'])))\n",
    "print(\"{:.4f}\".format(wv.n_similarity(['bulgogi', 'shop'], ['korean', 'restaurant'])))\n",
    "print(\"{:.4f}\".format(wv.n_similarity(['bulgogi', 'shop'], ['french', 'restaurant'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.3 FastText - 워드 임베딩에 N-gram 적용\n",
    "- Word2Vec의 단점 : 주변 단어를 중심으로 학습되어 문서 전체에서 다른 단어들과의 관계가 반영되지 않는다.\n",
    "- FastText : 단어에 문자 단위의 N-gram을 적용하여 해결\n",
    "- 모르는 단어에 대해서도 유사도를 계산하는 것이 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['어릴때보고 지금다시봐도 재밌어요ㅋㅋ', '디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업이 부러웠는데. 사실 우리나라에서도 그 어려운시절에 끝까지 열정을 지킨 노라노 같은 전통이있어 저와 같은 사람들이 꿈을 꾸고 이뤄나갈 수 있다는 것에 감사합니다.', '폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.', '와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런게 진짜 영화지', '안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.']\n",
      "[['어릴', '때', '보고', '지금', '다시', '봐도', '재밌어요', 'ㅋㅋ'], ['디자인', '을', '배우는', '학생', '으로', ',', '외국', '디자이너', '와', '그', '들', '이', '일군', '전통', '을', '통해', '발전', '해가는', '문화', '산업', '이', '부러웠는데', '.', '사실', '우리나라', '에서도', '그', '어려운', '시절', '에', '끝', '까지', '열정', '을', '지킨', '노라노', '같은', '전통', '이', '있어', '저', '와', '같은', '사람', '들', '이', '꿈', '을', '꾸고', '이뤄', '나갈', '수', '있다는', '것', '에', '감사합니다', '.'], ['폴리스스토리', '시리즈', '는', '1', '부터', '뉴', '까지', '버릴께', '하나', '도', '없음', '..', '최고', '.'], ['와', '..', '연기', '가', '진짜', '개', '쩔구나', '..', '지루할거라고', '생각', '했는데', '몰입', '해서', '봤다', '..', '그래', '이런게', '진짜', '영화', '지'], ['안개', '자욱한', '밤하늘', '에', '떠', '있는', '초승달', '같은', '영화', '.']]\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "raw = urllib.request.urlopen(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\").readlines()\n",
    "#print(raw[:5])\n",
    "\n",
    "raw = [x.decode() for x in raw[1:10000]]\n",
    "reviews = [i.split('\\t')[1] for i in raw]  # 첫 번째 리뷰만 출력\n",
    "print(reviews[:5])\n",
    "\n",
    "tagger = Okt()\n",
    "reviews = [tagger.morphs(x) for x in reviews]\n",
    "\n",
    "print(reviews[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 \n",
      "\n",
      "[-0.15216425 -0.00147272  0.14110371  0.01075903  0.09855263 -0.08730593\n",
      " -0.06043267  0.33225393 -0.5216205  -0.26063657 -0.13131046 -0.37680793\n",
      "  0.18187273  0.17356005  0.23740518 -0.18402278  0.2957796  -0.23837453\n",
      " -0.3497267  -0.38371336 -0.20431364 -0.42066967  0.14934638 -0.37225506\n",
      "  0.08345529 -0.21813664  0.07452591  0.0174174  -0.35617393 -0.0355469\n",
      " -0.0399428  -0.13010365  0.2819607  -0.20543161 -0.08716424 -0.08411624\n",
      "  0.28779462  0.32170516 -0.21999453 -0.22426105  0.12237037  0.19285753\n",
      " -0.38710698 -0.04337773 -0.26384395 -0.24698474 -0.11462265  0.15310797\n",
      "  0.04036434  0.00316472  0.3517563  -0.4296141  -0.42423397 -0.26468927\n",
      " -0.00428675 -0.17497107 -0.06764988 -0.23025762 -0.17269327  0.2605157\n",
      "  0.22100538  0.08673087  0.03740899  0.00080698 -0.03351995  0.24473312\n",
      "  0.14028043 -0.10880247 -0.40034607  0.07679953  0.18317643  0.08849915\n",
      "  0.08968691 -0.01451605  0.00410011  0.10251164 -0.21734513 -0.00921348\n",
      " -0.23055284 -0.19872567 -0.14994803  0.05334114 -0.2701748  -0.40284872\n",
      " -0.25856245  0.09242816 -0.21301551  0.02632102  0.2575673  -0.01137509\n",
      "  0.09745813 -0.02804544  0.2659287  -0.02024238 -0.11385529  0.32118636\n",
      " -0.21278702 -0.18137038 -0.12371448  0.11505781] \n",
      "\n",
      "[('드라마', 0.8962177038192749), ('작품', 0.8539049625396729), ('라마', 0.8218255639076233), ('애니', 0.8210970759391785), ('청춘영화', 0.8109592795372009), ('수작', 0.8101234436035156), ('영화계', 0.8041431307792664), ('진영화', 0.8018422722816467), ('영화사', 0.7996605634689331), ('멜로영화', 0.7952742576599121)] \n",
      "\n",
      "[('라마', 0.8940438032150269), ('애니', 0.8872536420822144), ('작품', 0.8808987736701965), ('청춘영화', 0.86972975730896), ('수작', 0.8623107075691223)] \n",
      "\n",
      "[('작품', 0.6744129657745361), ('명작', 0.6731906533241272), ('입니다', 0.5959814786911011), ('였다', 0.5888449549674988), ('수작', 0.5865454077720642)] \n",
      "\n",
      "바나나 \n",
      "\n",
      "0.6378339 \n",
      "\n",
      "0.3621661067008972 \n",
      "\n",
      "0.83031774\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "fasttext = FastText(\n",
    "    sentences=reviews,\n",
    "    sg=1,  # 1:skip-gram, 0:CBOW\n",
    "    vector_size=100,\n",
    "    window=3,  # 앞 뒤 몇 개의 단어까지 예측하는데 사용할 것인가?\n",
    "    min_count=3,  # 사용할 단어의 최소 빈도 수\n",
    "    workers=4  # 동시 처리 수\n",
    ")\n",
    "print(len(fasttext.wv[\"영화\"]), \"\\n\")\n",
    "print(fasttext.wv[\"영화\"], \"\\n\")  # 단어를 벡터로 변환\n",
    "\n",
    "print(fasttext.wv.most_similar(\"영화\"), \"\\n\")  # 가장 유사한 단어를 추출\n",
    "print(fasttext.wv.most_similar(positive=[\"영화\", \"드라마\"], topn=5), \"\\n\")\n",
    "print(fasttext.wv.most_similar(positive=[\"영화\", \"드라마\"], negative=[\"학생\"], topn=5), \"\\n\")\n",
    "\n",
    "print(fasttext.wv.doesnt_match(\"영화 드라마 작품 극장, 관객, 바나나\".split()), \"\\n\")  # 주어진 단어 집합 중 다른 단어들과 가장 거리가 먼 단어\n",
    "\n",
    "print(fasttext.wv.similarity(\"영화\", \"극장\"), \"\\n\")  # 영화, 극장 단어간의 거리(코사인 유사도)\n",
    "print(fasttext.wv.distance(\"영화\", \"극장\"), \"\\n\")  # similarity와 반대 개념의 거리\n",
    "print(fasttext.wv.n_similarity([\"드라마\", \"배우\"], [\"영화\", \"작품\"]))  #  단어 집합 간의 유사도 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMo - 문맥에 따른 단어 의미의 구분\n",
    "- 문맥을 반영한 위드 임베딩 모형 중 하나\n",
    "- Word2Vec : 임베딩 벡터가 고정\n",
    "- ELMo : 임베딩 벡터가 가변적\n",
    "- 사전 학습된 양방향 LSTM을 사용해 임베딩을 수행 : 정방향 LSTM과 역방향 LSTM은 독립적으로 학습된다.\n",
    "- 모형 자체를 전이하고 임베딩 벡터는 주어진 문장을 모형에 적용시켜서 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 Doc2Vec - 문맥을 고려한 문서 임베딩\n",
    "- 문서에 대해 직접 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n",
      "5 \n",
      "\n",
      "[-0.01072454  0.00472863  0.10206699  0.18018547 -0.186059  ] \n",
      "\n",
      "[('trees', 0.4593397080898285), ('graph', 0.2018294781446457), ('response', 0.11036164313554764), ('user', 0.10980252921581268), ('time', 0.04115491732954979), ('computer', 0.028436699882149696), ('eps', -0.0006994149298407137), ('minors', -0.11398053169250488), ('survey', -0.38140854239463806), ('interface', -0.4713294804096222)] \n",
      "\n",
      "[('trees', 0.4255462884902954), ('user', 0.3722703158855438), ('graph', 0.2598435878753662), ('survey', 0.19690008461475372), ('eps', -0.09318438917398453)] \n",
      "\n",
      "[('trees', 0.536517322063446), ('user', 0.3902610242366791), ('graph', 0.22985655069351196), ('eps', 0.05829356983304024), ('time', -0.018723903223872185)] \n",
      "\n",
      "system \n",
      "\n",
      "-0.13309234 \n",
      "\n",
      "1.1330923438072205 \n",
      "\n",
      "-0.713632\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "print(common_texts)\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "model = Doc2Vec(\n",
    "    documents=documents,\n",
    "    dm = 0,  # 1:DM(CBOW에 문서 ID를 추가한 형태) 0:DBOW(skip-gram에 문서 ID를 추가한 형태)\n",
    "    vector_size=5, \n",
    "    window=2, \n",
    "    min_count=1, \n",
    "    workers=4)\n",
    "\n",
    "print(len(model.wv[\"system\"]), \"\\n\")\n",
    "print(model.wv[\"system\"], \"\\n\")  # 단어를 벡터로 변환\n",
    "\n",
    "print(model.wv.most_similar(\"system\"), \"\\n\")  # 가장 유사한 단어를 추출\n",
    "print(model.wv.most_similar(positive=[\"system\", \"response\"], topn=5), \"\\n\")\n",
    "print(model.wv.most_similar(positive=[\"system\", \"response\"], negative=[\"human\"], topn=5), \"\\n\")\n",
    "\n",
    "print(model.wv.doesnt_match(\"system response human\".split()), \"\\n\")  # 주어진 단어 집합 중 다른 단어들과 가장 거리가 먼 단어\n",
    "\n",
    "print(model.wv.similarity(\"human\", \"computer\"), \"\\n\")\n",
    "print(model.wv.distance(\"human\", \"computer\"), \"\\n\")  # similarity와 반대 개념의 거리\n",
    "print(model.wv.n_similarity([\"human\", \"computer\"], [\"system\", \"response\"]))  #  단어 집합 간의 유사도 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
